{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.adapt import mlknn\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "      <th>synopsis_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0057603</td>\n",
       "      <td>I tre volti della paura</td>\n",
       "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
       "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1733125</td>\n",
       "      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n",
       "      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n",
       "      <td>violence</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0033045</td>\n",
       "      <td>The Shop Around the Corner</td>\n",
       "      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>test</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0113862</td>\n",
       "      <td>Mr. Holland's Opus</td>\n",
       "      <td>Glenn Holland, not a morning person by anyone'...</td>\n",
       "      <td>inspiring, romantic, stupid, feel-good</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0086250</td>\n",
       "      <td>Scarface</td>\n",
       "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
       "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
       "      <td>val</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     imdb_id                                          title  \\\n",
       "0  tt0057603                        I tre volti della paura   \n",
       "1  tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2  tt0033045                     The Shop Around the Corner   \n",
       "3  tt0113862                             Mr. Holland's Opus   \n",
       "4  tt0086250                                       Scarface   \n",
       "\n",
       "                                       plot_synopsis  \\\n",
       "0  Note: this synopsis is for the orginal Italian...   \n",
       "1  Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2  Matuschek's, a gift store in Budapest, is the ...   \n",
       "3  Glenn Holland, not a morning person by anyone'...   \n",
       "4  In May 1980, a Cuban man named Tony Montana (A...   \n",
       "\n",
       "                                                tags  split synopsis_source  \n",
       "0          cult, horror, gothic, murder, atmospheric  train            imdb  \n",
       "1                                           violence  train            imdb  \n",
       "2                                           romantic   test            imdb  \n",
       "3             inspiring, romantic, stupid, feel-good  train            imdb  \n",
       "4  cruelty, murder, dramatic, cult, violence, atm...    val            imdb  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mpst_full_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning the plots of the movies\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    #text = re.sub('\\W', ' ', text)\n",
    "    #text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "# function for text cleaning \n",
    "def cleaned(text):\n",
    "    # remove backslash-apostrophe \n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    # remove everything except alphabets \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    # remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # convert text to lowercase \n",
    "    text = text.lower() \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned'] = list(data['plot_synopsis'].apply(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned'] = list(data['cleaned'].apply(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ankan_rokr/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "data['cleaned'] = data['cleaned'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "      <th>synopsis_source</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0057603</td>\n",
       "      <td>I tre volti della paura</td>\n",
       "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
       "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "      <td>note synopsis orginal italian release segments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1733125</td>\n",
       "      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n",
       "      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n",
       "      <td>violence</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "      <td>two thousand years ago nhagruul foul sorcerer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0033045</td>\n",
       "      <td>The Shop Around the Corner</td>\n",
       "      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>test</td>\n",
       "      <td>imdb</td>\n",
       "      <td>matuschek gift store budapest workplace alfred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0113862</td>\n",
       "      <td>Mr. Holland's Opus</td>\n",
       "      <td>Glenn Holland, not a morning person by anyone'...</td>\n",
       "      <td>inspiring, romantic, stupid, feel-good</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "      <td>glenn holland morning person anyone standards ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0086250</td>\n",
       "      <td>Scarface</td>\n",
       "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
       "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
       "      <td>val</td>\n",
       "      <td>imdb</td>\n",
       "      <td>may cuban man named tony montana al pacino cla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     imdb_id                                          title  \\\n",
       "0  tt0057603                        I tre volti della paura   \n",
       "1  tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2  tt0033045                     The Shop Around the Corner   \n",
       "3  tt0113862                             Mr. Holland's Opus   \n",
       "4  tt0086250                                       Scarface   \n",
       "\n",
       "                                       plot_synopsis  \\\n",
       "0  Note: this synopsis is for the orginal Italian...   \n",
       "1  Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2  Matuschek's, a gift store in Budapest, is the ...   \n",
       "3  Glenn Holland, not a morning person by anyone'...   \n",
       "4  In May 1980, a Cuban man named Tony Montana (A...   \n",
       "\n",
       "                                                tags  split synopsis_source  \\\n",
       "0          cult, horror, gothic, murder, atmospheric  train            imdb   \n",
       "1                                           violence  train            imdb   \n",
       "2                                           romantic   test            imdb   \n",
       "3             inspiring, romantic, stupid, feel-good  train            imdb   \n",
       "4  cruelty, murder, dramatic, cult, violence, atm...    val            imdb   \n",
       "\n",
       "                                             cleaned  \n",
       "0  note synopsis orginal italian release segments...  \n",
       "1  two thousand years ago nhagruul foul sorcerer ...  \n",
       "2  matuschek gift store budapest workplace alfred...  \n",
       "3  glenn holland morning person anyone standards ...  \n",
       "4  may cuban man named tony montana al pacino cla...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  data['split']=='train'\n",
    "train = data[train_data]\n",
    "\n",
    "   \n",
    "    \n",
    "test_data =  data['split']=='test'\n",
    "test = data[test_data]\n",
    "\n",
    "  \n",
    "    \n",
    "validation_data =  data['split']=='val'\n",
    "val = data[validation_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with 15 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tags = CountVectorizer(tokenizer = lambda x: x.split(','), binary='true', max_features = 15)\n",
    "y_train = vectorizer_tags.fit_transform(train['tags'])\n",
    "y_test = vectorizer_tags.transform(test['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:04.237314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "vectorizer_unigram = TfidfVectorizer(min_df=5, max_features=20000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,1))\n",
    "x_train_multilabel_unigram = vectorizer_unigram.fit_transform(train['cleaned'])\n",
    "x_test_multilabel_unigram = vectorizer_unigram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.04888739042481457\n",
      "Hamming loss  0.22852326365475387\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2453, Recall: 0.4939, F1-measure: 0.3278\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2288, Recall: 0.4491, F1-measure: 0.2975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.47      0.31       351\n",
      "           1       0.24      0.44      0.31       515\n",
      "           2       0.11      0.27      0.15       150\n",
      "           3       0.49      0.64      0.55       885\n",
      "           4       0.15      0.39      0.22       229\n",
      "           5       0.19      0.46      0.27       268\n",
      "           6       0.18      0.36      0.24       311\n",
      "           7       0.37      0.65      0.48       593\n",
      "           8       0.14      0.35      0.20       248\n",
      "           9       0.11      0.27      0.15       200\n",
      "          10       0.18      0.41      0.25       270\n",
      "          11       0.41      0.44      0.43       166\n",
      "          12       0.17      0.39      0.24       239\n",
      "          13       0.27      0.67      0.39       276\n",
      "          14       0.20      0.50      0.28       318\n",
      "\n",
      "   micro avg       0.25      0.49      0.33      5019\n",
      "   macro avg       0.23      0.45      0.30      5019\n",
      "weighted avg       0.27      0.49      0.34      5019\n",
      " samples avg       0.23      0.41      0.27      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:02.223802\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel_unigram,y_train)\n",
    "predictions = classifier.predict (x_test_multilabel_unigram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:28.340605\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "vectorizer_bigram = TfidfVectorizer(min_df=5, max_features=20000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(2,2))\n",
    "x_train_multilabel_bigram = vectorizer_bigram.fit_transform(train['cleaned'])\n",
    "x_test_multilabel_bigram = vectorizer_bigram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.08664868509777478\n",
      "Hamming loss  0.18419869633625535\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2763, Recall: 0.3907, F1-measure: 0.3237\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2462, Recall: 0.3344, F1-measure: 0.2785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.37      0.29       351\n",
      "           1       0.27      0.39      0.32       515\n",
      "           2       0.16      0.19      0.18       150\n",
      "           3       0.48      0.58      0.52       885\n",
      "           4       0.18      0.23      0.20       229\n",
      "           5       0.22      0.36      0.27       268\n",
      "           6       0.19      0.31      0.24       311\n",
      "           7       0.39      0.55      0.46       593\n",
      "           8       0.16      0.26      0.20       248\n",
      "           9       0.11      0.13      0.12       200\n",
      "          10       0.17      0.24      0.20       270\n",
      "          11       0.47      0.31      0.37       166\n",
      "          12       0.16      0.19      0.17       239\n",
      "          13       0.29      0.56      0.38       276\n",
      "          14       0.20      0.36      0.26       318\n",
      "\n",
      "   micro avg       0.28      0.39      0.32      5019\n",
      "   macro avg       0.25      0.33      0.28      5019\n",
      "weighted avg       0.28      0.39      0.33      5019\n",
      " samples avg       0.22      0.31      0.23      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.731417\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel_bigram,y_train)\n",
    "predictions = classifier.predict (x_test_multilabel_bigram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:40.036978\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vectorizer_trigram = TfidfVectorizer(min_df=10, max_features=20000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(3,3))\n",
    "x_train_multilabel_trigram = vectorizer_trigram.fit_transform(train['cleaned'])\n",
    "x_test_multilabel_trigram = vectorizer_trigram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0684423465947404\n",
      "Hamming loss  0.2823331085637222\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1540, Recall: 0.3343, F1-measure: 0.2108\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1428, Recall: 0.3102, F1-measure: 0.1898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.33      0.20       351\n",
      "           1       0.22      0.39      0.28       515\n",
      "           2       0.07      0.31      0.12       150\n",
      "           3       0.37      0.40      0.39       885\n",
      "           4       0.09      0.32      0.14       229\n",
      "           5       0.13      0.35      0.19       268\n",
      "           6       0.13      0.36      0.20       311\n",
      "           7       0.26      0.38      0.31       593\n",
      "           8       0.09      0.29      0.14       248\n",
      "           9       0.07      0.19      0.10       200\n",
      "          10       0.13      0.29      0.18       270\n",
      "          11       0.07      0.20      0.11       166\n",
      "          12       0.08      0.21      0.11       239\n",
      "          13       0.16      0.40      0.23       276\n",
      "          14       0.12      0.25      0.16       318\n",
      "\n",
      "   micro avg       0.15      0.33      0.21      5019\n",
      "   macro avg       0.14      0.31      0.19      5019\n",
      "weighted avg       0.19      0.33      0.23      5019\n",
      " samples avg       0.10      0.26      0.13      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.101363\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel_trigram,y_train)\n",
    "predictions = classifier.predict (x_test_multilabel_trigram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:41.245621\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vectorizer_fourgram = TfidfVectorizer(min_df=10, max_features=20000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(4,4))\n",
    "x_train_multilabel_fourgram = vectorizer_fourgram.fit_transform(train['cleaned'])\n",
    "x_test_multilabel_fourgram = vectorizer_fourgram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0\n",
      "Hamming loss  0.3870532703978422\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0915, Recall: 0.2724, F1-measure: 0.1370\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1718, Recall: 0.3425, F1-measure: 0.0787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.03      0.06       351\n",
      "           1       0.37      0.02      0.04       515\n",
      "           2       0.08      0.01      0.02       150\n",
      "           3       0.52      0.03      0.05       885\n",
      "           4       0.09      0.02      0.03       229\n",
      "           5       0.10      0.02      0.03       268\n",
      "           6       0.10      0.98      0.19       311\n",
      "           7       0.24      0.02      0.04       593\n",
      "           8       0.13      0.01      0.02       248\n",
      "           9       0.15      0.04      0.06       200\n",
      "          10       0.09      0.98      0.17       270\n",
      "          11       0.06      0.98      0.11       166\n",
      "          12       0.08      0.99      0.15       239\n",
      "          13       0.16      0.01      0.03       276\n",
      "          14       0.11      0.99      0.19       318\n",
      "\n",
      "   micro avg       0.09      0.27      0.14      5019\n",
      "   macro avg       0.17      0.34      0.08      5019\n",
      "weighted avg       0.24      0.27      0.07      5019\n",
      " samples avg       0.09      0.30      0.13      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.098582\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel_fourgram,y_train)\n",
    "predictions = classifier.predict (x_test_multilabel_fourgram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:02:26.303597\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vectorizer_ngram = TfidfVectorizer(min_df=10, max_features=20000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,4))\n",
    "x_train_multilabel_ngram = vectorizer_ngram.fit_transform(train['cleaned'])\n",
    "x_test_multilabel_ngram = vectorizer_ngram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.04686446392447741\n",
      "Hamming loss  0.23429984266127218\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2437, Recall: 0.5119, F1-measure: 0.3302\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2294, Recall: 0.4699, F1-measure: 0.3023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.50      0.31       351\n",
      "           1       0.25      0.47      0.33       515\n",
      "           2       0.12      0.33      0.18       150\n",
      "           3       0.49      0.65      0.56       885\n",
      "           4       0.15      0.40      0.22       229\n",
      "           5       0.19      0.49      0.27       268\n",
      "           6       0.18      0.38      0.24       311\n",
      "           7       0.38      0.66      0.49       593\n",
      "           8       0.14      0.38      0.21       248\n",
      "           9       0.10      0.28      0.15       200\n",
      "          10       0.18      0.43      0.25       270\n",
      "          11       0.40      0.46      0.43       166\n",
      "          12       0.17      0.44      0.25       239\n",
      "          13       0.27      0.67      0.39       276\n",
      "          14       0.19      0.52      0.28       318\n",
      "\n",
      "   micro avg       0.24      0.51      0.33      5019\n",
      "   macro avg       0.23      0.47      0.30      5019\n",
      "weighted avg       0.27      0.51      0.35      5019\n",
      " samples avg       0.23      0.42      0.27      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.766240\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel_ngram,y_train)\n",
    "predictions = classifier.predict (x_test_multilabel_ngram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = hstack((x_train_multilabel_unigram, x_train_multilabel_bigram, x_train_multilabel_trigram),format=\"csr\",dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_1 = hstack((x_test_multilabel_unigram, x_test_multilabel_bigram, x_test_multilabel_trigram),format=\"csr\",dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30083370327655323\n",
      "{'estimator__alpha': 0.01}\n",
      "Time taken to run this cell : 0:00:05.205670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "start = datetime.now()\n",
    "\n",
    "model = OneVsRestClassifier(SGDClassifier(loss='log', penalty='l2', class_weight=\"balanced\", n_jobs = -1 ), n_jobs = -1)\n",
    "\n",
    "param_grid = {\n",
    "    \"estimator__alpha\": [10**-4,10**-3, 10**-2,10**-1, 10**0, 10**1, 10**2, 10**3, 10**4]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(model, param_grid, scoring='f1_micro',n_jobs=-1)\n",
    "\n",
    "model.fit(x_train_1, y_train)\n",
    "\n",
    "print (model.best_score_)\n",
    "print (model.best_params_)\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.07687120701281187\n",
      "Hamming loss  0.19150370869858396\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2712, Recall: 0.4134, F1-measure: 0.3275\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2420, Recall: 0.3600, F1-measure: 0.2862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.39      0.30       351\n",
      "           1       0.27      0.38      0.32       515\n",
      "           2       0.14      0.24      0.18       150\n",
      "           3       0.50      0.62      0.56       885\n",
      "           4       0.16      0.26      0.20       229\n",
      "           5       0.20      0.33      0.25       268\n",
      "           6       0.19      0.30      0.23       311\n",
      "           7       0.39      0.56      0.46       593\n",
      "           8       0.14      0.23      0.17       248\n",
      "           9       0.10      0.16      0.12       200\n",
      "          10       0.19      0.33      0.24       270\n",
      "          11       0.45      0.40      0.43       166\n",
      "          12       0.16      0.23      0.19       239\n",
      "          13       0.30      0.60      0.40       276\n",
      "          14       0.18      0.36      0.24       318\n",
      "\n",
      "   micro avg       0.27      0.41      0.33      5019\n",
      "   macro avg       0.24      0.36      0.29      5019\n",
      "weighted avg       0.29      0.41      0.34      5019\n",
      " samples avg       0.23      0.33      0.25      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.868872\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_1, y_train)\n",
    "predictions = classifier.predict(x_test_1)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = hstack((x_train_multilabel_unigram, x_train_multilabel_bigram, x_train_multilabel_trigram, x_train_multilabel_fourgram),format=\"csr\",dtype='float64')\n",
    "x_test_2 = hstack((x_test_multilabel_unigram, x_test_multilabel_bigram, x_test_multilabel_trigram, x_test_multilabel_fourgram),format=\"csr\",dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29446349584195286\n",
      "{'estimator__alpha': 0.01}\n",
      "Time taken to run this cell : 0:00:05.681882\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "model = OneVsRestClassifier(SGDClassifier(loss='log', penalty='l2', class_weight=\"balanced\", n_jobs = -1 ), n_jobs = -1)\n",
    "\n",
    "param_grid = {\n",
    "    \"estimator__alpha\": [10**-4,10**-3, 10**-2,10**-1, 10**0, 10**1, 10**2, 10**3, 10**4]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(model, param_grid, scoring='f1_micro',n_jobs=-1)\n",
    "\n",
    "model.fit(x_train_2, y_train)\n",
    "\n",
    "print (model.best_score_)\n",
    "print (model.best_params_)\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.07484828051247472\n",
      "Hamming loss  0.19154866262081366\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2708, Recall: 0.4124, F1-measure: 0.3270\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2396, Recall: 0.3583, F1-measure: 0.2844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.41      0.31       351\n",
      "           1       0.27      0.39      0.32       515\n",
      "           2       0.14      0.23      0.18       150\n",
      "           3       0.50      0.62      0.55       885\n",
      "           4       0.16      0.27      0.20       229\n",
      "           5       0.19      0.32      0.24       268\n",
      "           6       0.19      0.28      0.23       311\n",
      "           7       0.39      0.56      0.46       593\n",
      "           8       0.14      0.24      0.17       248\n",
      "           9       0.09      0.14      0.11       200\n",
      "          10       0.19      0.32      0.24       270\n",
      "          11       0.42      0.40      0.41       166\n",
      "          12       0.16      0.22      0.18       239\n",
      "          13       0.30      0.61      0.41       276\n",
      "          14       0.18      0.35      0.24       318\n",
      "\n",
      "   micro avg       0.27      0.41      0.33      5019\n",
      "   macro avg       0.24      0.36      0.28      5019\n",
      "weighted avg       0.28      0.41      0.33      5019\n",
      " samples avg       0.23      0.33      0.25      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.842880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_2, y_train)\n",
    "predictions = classifier.predict(x_test_2)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glove = np.load('glove_tarin.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glove = np.load('glove_test.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = hstack((x_train_multilabel_unigram, x_train_multilabel_bigram, x_train_multilabel_trigram, x_train_multilabel_fourgram),format=\"csr\",dtype='float64')\n",
    "x_test_2 = hstack((x_test_multilabel_unigram, x_test_multilabel_bigram, x_test_multilabel_trigram, x_test_multilabel_fourgram),format=\"csr\",dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2 = x_train_2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 40857)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2 = x_test_2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966, 40857)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_3 = np.hstack((x_train_2, train_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 41157)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_3 = np.hstack((x_test_2, test_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966, 41157)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gove+train2.npy', x_train_3)\n",
    "np.save('gove+test2.npy', x_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.08125421443020904\n",
      "Hamming loss  0.18314227916385706\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3486, Recall: 0.7179, F1-measure: 0.4693\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3368, Recall: 0.6919, F1-measure: 0.4436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.66      0.44       351\n",
      "           1       0.53      0.87      0.66       515\n",
      "           2       0.20      0.61      0.31       150\n",
      "           3       0.57      0.75      0.65       885\n",
      "           4       0.20      0.63      0.31       229\n",
      "           5       0.33      0.79      0.46       268\n",
      "           6       0.30      0.74      0.43       311\n",
      "           7       0.53      0.76      0.62       593\n",
      "           8       0.25      0.70      0.36       248\n",
      "           9       0.23      0.40      0.29       200\n",
      "          10       0.36      0.76      0.49       270\n",
      "          11       0.50      0.70      0.58       166\n",
      "          12       0.17      0.59      0.27       239\n",
      "          13       0.26      0.80      0.40       276\n",
      "          14       0.28      0.62      0.39       318\n",
      "\n",
      "   micro avg       0.35      0.72      0.47      5019\n",
      "   macro avg       0.34      0.69      0.44      5019\n",
      "weighted avg       0.39      0.72      0.50      5019\n",
      " samples avg       0.33      0.62      0.41      5019\n",
      "\n",
      "Time taken to run this cell : 0:01:13.610934\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs=-1)\n",
    "classifier.fit(x_train_3, y_train)\n",
    "predictions = classifier.predict(x_test_3)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.09743762643290627\n",
      "Hamming loss  0.174645987862441\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3595, Recall: 0.7011, F1-measure: 0.4753\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3409, Recall: 0.6613, F1-measure: 0.4438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.67      0.41       351\n",
      "           1       0.56      0.85      0.68       515\n",
      "           2       0.24      0.53      0.33       150\n",
      "           3       0.53      0.78      0.63       885\n",
      "           4       0.23      0.47      0.31       229\n",
      "           5       0.45      0.69      0.54       268\n",
      "           6       0.30      0.73      0.43       311\n",
      "           7       0.51      0.79      0.62       593\n",
      "           8       0.23      0.65      0.34       248\n",
      "           9       0.21      0.47      0.29       200\n",
      "          10       0.27      0.83      0.41       270\n",
      "          11       0.41      0.67      0.51       166\n",
      "          12       0.20      0.58      0.30       239\n",
      "          13       0.38      0.63      0.48       276\n",
      "          14       0.29      0.60      0.40       318\n",
      "\n",
      "   micro avg       0.36      0.70      0.48      5019\n",
      "   macro avg       0.34      0.66      0.44      5019\n",
      "weighted avg       0.39      0.70      0.49      5019\n",
      " samples avg       0.34      0.60      0.41      5019\n",
      "\n",
      "Time taken to run this cell : 0:13:09.389036\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.001, penalty='l2', class_weight=\"balanced\"))\n",
    "classifier.fit(x_train_3, y_train)\n",
    "predictions = classifier.predict(x_test_3)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = x_train_1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_1 = x_test_1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_4 = np.hstack((x_train_1, train_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_4 = np.hstack((x_test_1, test_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('gove+train1.npy', x_train_4)\n",
    "np.save('gove+test1.npy', x_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.08496291301416048\n",
      "Hamming loss  0.18152393796358732\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3501, Recall: 0.7113, F1-measure: 0.4692\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3323, Recall: 0.6786, F1-measure: 0.4394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.67      0.42       351\n",
      "           1       0.50      0.91      0.65       515\n",
      "           2       0.20      0.65      0.31       150\n",
      "           3       0.57      0.76      0.65       885\n",
      "           4       0.19      0.60      0.29       229\n",
      "           5       0.40      0.73      0.51       268\n",
      "           6       0.30      0.73      0.43       311\n",
      "           7       0.53      0.77      0.62       593\n",
      "           8       0.21      0.69      0.33       248\n",
      "           9       0.20      0.47      0.28       200\n",
      "          10       0.38      0.74      0.50       270\n",
      "          11       0.42      0.72      0.53       166\n",
      "          12       0.22      0.44      0.30       239\n",
      "          13       0.30      0.66      0.42       276\n",
      "          14       0.23      0.65      0.34       318\n",
      "\n",
      "   micro avg       0.35      0.71      0.47      5019\n",
      "   macro avg       0.33      0.68      0.44      5019\n",
      "weighted avg       0.39      0.71      0.49      5019\n",
      " samples avg       0.35      0.61      0.40      5019\n",
      "\n",
      "Time taken to run this cell : 0:10:51.064275\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"))\n",
    "classifier.fit(x_train_4, y_train)\n",
    "predictions = classifier.predict(x_test_4)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.05933917734322319\n",
      "Hamming loss  0.19082939986513822\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3352, Recall: 0.7031, F1-measure: 0.4539\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3353, Recall: 0.6869, F1-measure: 0.4380\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.72      0.41       351\n",
      "           1       0.58      0.84      0.69       515\n",
      "           2       0.20      0.65      0.31       150\n",
      "           3       0.59      0.65      0.62       885\n",
      "           4       0.20      0.52      0.29       229\n",
      "           5       0.35      0.75      0.47       268\n",
      "           6       0.35      0.66      0.46       311\n",
      "           7       0.46      0.81      0.59       593\n",
      "           8       0.21      0.69      0.32       248\n",
      "           9       0.16      0.62      0.25       200\n",
      "          10       0.42      0.75      0.54       270\n",
      "          11       0.40      0.66      0.50       166\n",
      "          12       0.18      0.64      0.28       239\n",
      "          13       0.42      0.63      0.50       276\n",
      "          14       0.22      0.72      0.34       318\n",
      "\n",
      "   micro avg       0.34      0.70      0.45      5019\n",
      "   macro avg       0.34      0.69      0.44      5019\n",
      "weighted avg       0.39      0.70      0.49      5019\n",
      " samples avg       0.31      0.61      0.38      5019\n",
      "\n",
      "Time taken to run this cell : 0:11:21.781075\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.001, penalty='l2', class_weight=\"balanced\", n_jobs = -1))\n",
    "classifier.fit(x_train_4, y_train)\n",
    "predictions = classifier.predict(x_test_4)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_5 = np.hstack((x_train_4, x_train_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_5 = np.hstack((x_test_4, x_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('all1.npy', x_train_5)\n",
    "np.save('all2.npy', x_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0832771409305462\n",
      "Hamming loss  0.16174421218251292\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3789, Recall: 0.6788, F1-measure: 0.4864\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3609, Recall: 0.6495, F1-measure: 0.4515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.71      0.42       351\n",
      "           1       0.64      0.81      0.72       515\n",
      "           2       0.15      0.71      0.25       150\n",
      "           3       0.59      0.72      0.65       885\n",
      "           4       0.22      0.50      0.31       229\n",
      "           5       0.38      0.75      0.50       268\n",
      "           6       0.35      0.69      0.46       311\n",
      "           7       0.52      0.76      0.62       593\n",
      "           8       0.24      0.62      0.35       248\n",
      "           9       0.33      0.34      0.33       200\n",
      "          10       0.37      0.76      0.50       270\n",
      "          11       0.33      0.71      0.45       166\n",
      "          12       0.32      0.33      0.33       239\n",
      "          13       0.38      0.71      0.50       276\n",
      "          14       0.28      0.61      0.39       318\n",
      "\n",
      "   micro avg       0.38      0.68      0.49      5019\n",
      "   macro avg       0.36      0.65      0.45      5019\n",
      "weighted avg       0.42      0.68      0.51      5019\n",
      " samples avg       0.37      0.59      0.42      5019\n",
      "\n",
      "Time taken to run this cell : 0:02:39.677757\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_5, y_train)\n",
    "predictions = classifier.predict(x_test_5)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.15677680377612946\n",
      "Hamming loss  0.13011912789390875\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4417, Recall: 0.5808, F1-measure: 0.5018\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4260, Recall: 0.5289, F1-measure: 0.4610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.50      0.41       351\n",
      "           1       0.64      0.80      0.71       515\n",
      "           2       0.34      0.37      0.36       150\n",
      "           3       0.58      0.72      0.64       885\n",
      "           4       0.24      0.37      0.29       229\n",
      "           5       0.29      0.71      0.41       268\n",
      "           6       0.44      0.51      0.47       311\n",
      "           7       0.55      0.64      0.59       593\n",
      "           8       0.30      0.45      0.36       248\n",
      "           9       0.44      0.24      0.31       200\n",
      "          10       0.41      0.69      0.51       270\n",
      "          11       0.76      0.60      0.67       166\n",
      "          12       0.26      0.30      0.27       239\n",
      "          13       0.48      0.61      0.53       276\n",
      "          14       0.33      0.44      0.38       318\n",
      "\n",
      "   micro avg       0.44      0.58      0.50      5019\n",
      "   macro avg       0.43      0.53      0.46      5019\n",
      "weighted avg       0.46      0.58      0.51      5019\n",
      " samples avg       0.39      0.51      0.41      5019\n",
      "\n",
      "Time taken to run this cell : 0:02:46.941117\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.0001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_5, y_train)\n",
    "predictions = classifier.predict(x_test_5)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.21072151045178691\n",
      "Hamming loss  0.1135760845133738\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4959, Recall: 0.4144, F1-measure: 0.4515\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4417, Recall: 0.3552, F1-measure: 0.3819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.26      0.34       351\n",
      "           1       0.68      0.65      0.67       515\n",
      "           2       0.49      0.23      0.32       150\n",
      "           3       0.57      0.64      0.61       885\n",
      "           4       0.37      0.17      0.24       229\n",
      "           5       0.49      0.34      0.41       268\n",
      "           6       0.44      0.32      0.37       311\n",
      "           7       0.62      0.51      0.56       593\n",
      "           8       0.36      0.21      0.27       248\n",
      "           9       0.31      0.28      0.29       200\n",
      "          10       0.45      0.28      0.35       270\n",
      "          11       0.30      0.56      0.39       166\n",
      "          12       0.26      0.21      0.23       239\n",
      "          13       0.51      0.49      0.50       276\n",
      "          14       0.31      0.15      0.20       318\n",
      "\n",
      "   micro avg       0.50      0.41      0.45      5019\n",
      "   macro avg       0.44      0.36      0.38      5019\n",
      "weighted avg       0.49      0.41      0.44      5019\n",
      " samples avg       0.38      0.35      0.34      5019\n",
      "\n",
      "Time taken to run this cell : 0:04:37.393199\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_5, y_train)\n",
    "predictions = classifier.predict(x_test_5)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.11598111935266352\n",
      "Hamming loss  0.14495392222971454\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4096, Recall: 0.6455, F1-measure: 0.5012\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3929, Recall: 0.6023, F1-measure: 0.4663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.61      0.44       351\n",
      "           1       0.59      0.83      0.69       515\n",
      "           2       0.23      0.47      0.31       150\n",
      "           3       0.61      0.69      0.65       885\n",
      "           4       0.24      0.43      0.30       229\n",
      "           5       0.41      0.67      0.51       268\n",
      "           6       0.36      0.65      0.47       311\n",
      "           7       0.54      0.78      0.64       593\n",
      "           8       0.28      0.54      0.37       248\n",
      "           9       0.22      0.42      0.29       200\n",
      "          10       0.42      0.74      0.53       270\n",
      "          11       0.65      0.60      0.62       166\n",
      "          12       0.34      0.28      0.31       239\n",
      "          13       0.42      0.67      0.51       276\n",
      "          14       0.24      0.66      0.35       318\n",
      "\n",
      "   micro avg       0.41      0.65      0.50      5019\n",
      "   macro avg       0.39      0.60      0.47      5019\n",
      "weighted avg       0.44      0.65      0.51      5019\n",
      " samples avg       0.37      0.57      0.42      5019\n",
      "\n",
      "Time taken to run this cell : 0:02:50.718128\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_5, y_train)\n",
    "predictions = classifier.predict(x_test_5)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.19453809844908967\n",
      "Hamming loss  0.11692515171948753\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4810, Recall: 0.4620, F1-measure: 0.4713\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4324, Recall: 0.4278, F1-measure: 0.4221\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.27      0.35       351\n",
      "           1       0.69      0.74      0.71       515\n",
      "           2       0.23      0.19      0.21       150\n",
      "           3       0.58      0.54      0.56       885\n",
      "           4       0.29      0.24      0.26       229\n",
      "           5       0.44      0.59      0.50       268\n",
      "           6       0.43      0.51      0.47       311\n",
      "           7       0.61      0.50      0.55       593\n",
      "           8       0.36      0.33      0.34       248\n",
      "           9       0.37      0.23      0.28       200\n",
      "          10       0.46      0.44      0.45       270\n",
      "          11       0.55      0.67      0.61       166\n",
      "          12       0.27      0.41      0.32       239\n",
      "          13       0.45      0.60      0.51       276\n",
      "          14       0.30      0.16      0.21       318\n",
      "\n",
      "   micro avg       0.48      0.46      0.47      5019\n",
      "   macro avg       0.43      0.43      0.42      5019\n",
      "weighted avg       0.48      0.46      0.46      5019\n",
      " samples avg       0.40      0.41      0.37      5019\n",
      "\n",
      "Time taken to run this cell : 0:03:26.365870\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.0001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_5, y_train)\n",
    "predictions = classifier.predict(x_test_5)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:03:33.371687\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(1, 1),  max_features=20000)\n",
    "x_train_char = vectorizer_char.fit_transform(train['cleaned'])\n",
    "x_test_char = vectorizer_char.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.0\n",
      "Hamming loss  0.3751854349291976\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1506, Recall: 0.5013, F1-measure: 0.2316\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1373, Recall: 0.4702, F1-measure: 0.1808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.77      0.25       351\n",
      "           1       0.23      0.26      0.24       515\n",
      "           2       0.07      0.67      0.13       150\n",
      "           3       0.31      0.82      0.45       885\n",
      "           4       0.10      0.56      0.18       229\n",
      "           5       0.11      0.69      0.20       268\n",
      "           6       0.15      0.18      0.17       311\n",
      "           7       0.28      0.30      0.29       593\n",
      "           8       0.10      0.74      0.18       248\n",
      "           9       0.05      0.01      0.01       200\n",
      "          10       0.00      0.00      0.00       270\n",
      "          11       0.12      0.47      0.19       166\n",
      "          12       0.15      0.02      0.04       239\n",
      "          13       0.13      0.57      0.21       276\n",
      "          14       0.11      1.00      0.19       318\n",
      "\n",
      "   micro avg       0.15      0.50      0.23      5019\n",
      "   macro avg       0.14      0.47      0.18      5019\n",
      "weighted avg       0.18      0.50      0.23      5019\n",
      " samples avg       0.15      0.38      0.20      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.156325\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_char, y_train)\n",
    "predictions = classifier.predict(x_test_char)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:16.204881\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char_2 = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(2, 2),  max_features=20000)\n",
    "x_train_char_2 = vectorizer_char_2.fit_transform(train['cleaned'])\n",
    "x_test_char_2 = vectorizer_char_2.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.010114632501685773\n",
      "Hamming loss  0.3314452685996853\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1836, Recall: 0.5621, F1-measure: 0.2767\n",
      "Macro-average quality numbers\n",
      "Precision: 0.1776, Recall: 0.5486, F1-measure: 0.2568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.56      0.31       351\n",
      "           1       0.26      0.43      0.32       515\n",
      "           2       0.08      0.60      0.14       150\n",
      "           3       0.41      0.65      0.50       885\n",
      "           4       0.13      0.44      0.20       229\n",
      "           5       0.17      0.50      0.26       268\n",
      "           6       0.16      0.58      0.25       311\n",
      "           7       0.31      0.60      0.41       593\n",
      "           8       0.12      0.58      0.20       248\n",
      "           9       0.10      0.13      0.12       200\n",
      "          10       0.11      0.72      0.20       270\n",
      "          11       0.11      0.71      0.20       166\n",
      "          12       0.11      0.50      0.19       239\n",
      "          13       0.20      0.62      0.31       276\n",
      "          14       0.17      0.59      0.26       318\n",
      "\n",
      "   micro avg       0.18      0.56      0.28      5019\n",
      "   macro avg       0.18      0.55      0.26      5019\n",
      "weighted avg       0.22      0.56      0.31      5019\n",
      " samples avg       0.17      0.46      0.24      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:00.560678\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_char_2, y_train)\n",
    "predictions = classifier.predict(x_test_char_2)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:21.718450\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char_3 = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(3, 3),  max_features=20000)\n",
    "x_train_char_3 = vectorizer_char_3.fit_transform(train['cleaned'])\n",
    "x_test_char_3 = vectorizer_char_3.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.028658125421443022\n",
      "Hamming loss  0.2658125421443021\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2346, Recall: 0.5993, F1-measure: 0.3372\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2211, Recall: 0.5724, F1-measure: 0.3130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.64      0.33       351\n",
      "           1       0.28      0.54      0.37       515\n",
      "           2       0.12      0.44      0.19       150\n",
      "           3       0.49      0.68      0.57       885\n",
      "           4       0.15      0.53      0.23       229\n",
      "           5       0.19      0.59      0.29       268\n",
      "           6       0.19      0.62      0.29       311\n",
      "           7       0.36      0.70      0.48       593\n",
      "           8       0.16      0.54      0.24       248\n",
      "           9       0.11      0.45      0.18       200\n",
      "          10       0.17      0.49      0.25       270\n",
      "          11       0.27      0.61      0.37       166\n",
      "          12       0.16      0.51      0.24       239\n",
      "          13       0.26      0.70      0.38       276\n",
      "          14       0.20      0.54      0.29       318\n",
      "\n",
      "   micro avg       0.23      0.60      0.34      5019\n",
      "   macro avg       0.22      0.57      0.31      5019\n",
      "weighted avg       0.27      0.60      0.36      5019\n",
      " samples avg       0.23      0.49      0.28      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:01.560151\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"),n_jobs = -1)\n",
    "classifier.fit(x_train_char_3, y_train)\n",
    "predictions = classifier.predict(x_test_char_3)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:30.628558\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char_4 = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(4, 4),  max_features=20000)\n",
    "x_train_char_4 = vectorizer_char_4.fit_transform(train['cleaned'])\n",
    "x_test_char_4 = vectorizer_char_4.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.034726904922454484\n",
      "Hamming loss  0.25830523713193976\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2424, Recall: 0.6069, F1-measure: 0.3465\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2327, Recall: 0.5787, F1-measure: 0.3248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.63      0.34       351\n",
      "           1       0.27      0.62      0.37       515\n",
      "           2       0.12      0.51      0.19       150\n",
      "           3       0.50      0.69      0.58       885\n",
      "           4       0.15      0.53      0.23       229\n",
      "           5       0.20      0.60      0.30       268\n",
      "           6       0.20      0.50      0.29       311\n",
      "           7       0.38      0.69      0.49       593\n",
      "           8       0.16      0.49      0.24       248\n",
      "           9       0.12      0.47      0.19       200\n",
      "          10       0.17      0.56      0.27       270\n",
      "          11       0.35      0.58      0.43       166\n",
      "          12       0.18      0.49      0.26       239\n",
      "          13       0.27      0.72      0.40       276\n",
      "          14       0.19      0.59      0.29       318\n",
      "\n",
      "   micro avg       0.24      0.61      0.35      5019\n",
      "   macro avg       0.23      0.58      0.32      5019\n",
      "weighted avg       0.28      0.61      0.37      5019\n",
      " samples avg       0.24      0.49      0.29      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:02.433163\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_char_4, y_train)\n",
    "predictions = classifier.predict(x_test_char_4)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:40.414777\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char_5 = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(5, 5),  max_features=20000)\n",
    "x_train_char_5 = vectorizer_char_5.fit_transform(train['cleaned'])\n",
    "x_test_char_5 = vectorizer_char_5.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.037086985839514496\n",
      "Hamming loss  0.2539896605978872\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2452, Recall: 0.6023, F1-measure: 0.3486\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2318, Recall: 0.5707, F1-measure: 0.3236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.62      0.34       351\n",
      "           1       0.27      0.58      0.37       515\n",
      "           2       0.13      0.55      0.21       150\n",
      "           3       0.49      0.71      0.58       885\n",
      "           4       0.15      0.52      0.23       229\n",
      "           5       0.20      0.63      0.30       268\n",
      "           6       0.21      0.51      0.30       311\n",
      "           7       0.39      0.72      0.50       593\n",
      "           8       0.16      0.46      0.24       248\n",
      "           9       0.12      0.40      0.18       200\n",
      "          10       0.18      0.46      0.26       270\n",
      "          11       0.33      0.57      0.41       166\n",
      "          12       0.17      0.51      0.25       239\n",
      "          13       0.27      0.73      0.40       276\n",
      "          14       0.19      0.59      0.28       318\n",
      "\n",
      "   micro avg       0.25      0.60      0.35      5019\n",
      "   macro avg       0.23      0.57      0.32      5019\n",
      "weighted avg       0.28      0.60      0.37      5019\n",
      " samples avg       0.24      0.48      0.29      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:01.953407\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_char_5, y_train)\n",
    "predictions = classifier.predict(x_test_char_5)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:53.580652\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char_6 = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(6, 6),  max_features=20000)\n",
    "x_train_char_6 = vectorizer_char_6.fit_transform(train['cleaned'])\n",
    "x_test_char_6 = vectorizer_char_6.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.03405259608900876\n",
      "Hamming loss  0.252596089008766\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2448, Recall: 0.5943, F1-measure: 0.3468\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2315, Recall: 0.5653, F1-measure: 0.3222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.60      0.33       351\n",
      "           1       0.28      0.57      0.38       515\n",
      "           2       0.13      0.51      0.21       150\n",
      "           3       0.50      0.68      0.58       885\n",
      "           4       0.15      0.51      0.23       229\n",
      "           5       0.20      0.65      0.30       268\n",
      "           6       0.21      0.54      0.30       311\n",
      "           7       0.39      0.71      0.50       593\n",
      "           8       0.16      0.49      0.24       248\n",
      "           9       0.11      0.41      0.17       200\n",
      "          10       0.18      0.50      0.26       270\n",
      "          11       0.30      0.56      0.39       166\n",
      "          12       0.17      0.47      0.25       239\n",
      "          13       0.27      0.72      0.39       276\n",
      "          14       0.19      0.58      0.29       318\n",
      "\n",
      "   micro avg       0.24      0.59      0.35      5019\n",
      "   macro avg       0.23      0.57      0.32      5019\n",
      "weighted avg       0.28      0.59      0.37      5019\n",
      " samples avg       0.23      0.48      0.29      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:01.680872\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_char_6, y_train)\n",
    "predictions = classifier.predict(x_test_char_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:01:18.730078\n"
     ]
    }
   ],
   "source": [
    "vectorizer_char_8 = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(7, 7),  max_features=20000)\n",
    "x_train_char_8 = vectorizer_char_8.fit_transform(train['cleaned'])\n",
    "x_test_char_8 = vectorizer_char_8.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.03607552258934592\n",
      "Hamming loss  0.24747134187457856\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2483, Recall: 0.5888, F1-measure: 0.3493\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2332, Recall: 0.5581, F1-measure: 0.3228\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.58      0.33       351\n",
      "           1       0.30      0.57      0.39       515\n",
      "           2       0.14      0.53      0.22       150\n",
      "           3       0.50      0.68      0.58       885\n",
      "           4       0.15      0.48      0.23       229\n",
      "           5       0.21      0.63      0.32       268\n",
      "           6       0.20      0.52      0.29       311\n",
      "           7       0.40      0.71      0.51       593\n",
      "           8       0.16      0.52      0.25       248\n",
      "           9       0.11      0.34      0.16       200\n",
      "          10       0.18      0.50      0.26       270\n",
      "          11       0.30      0.54      0.38       166\n",
      "          12       0.17      0.46      0.25       239\n",
      "          13       0.27      0.73      0.39       276\n",
      "          14       0.19      0.58      0.29       318\n",
      "\n",
      "   micro avg       0.25      0.59      0.35      5019\n",
      "   macro avg       0.23      0.56      0.32      5019\n",
      "weighted avg       0.28      0.59      0.37      5019\n",
      " samples avg       0.23      0.48      0.29      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:01.287939\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_char_8, y_train)\n",
    "predictions = classifier.predict(x_test_char_8)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:03:19.747493\n"
     ]
    }
   ],
   "source": [
    "vectorizer_charngram = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(1, 6),  max_features=20000)\n",
    "x_train_charngram = vectorizer_charngram.fit_transform(train['cleaned'])\n",
    "x_test_charngram = vectorizer_charngram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.030681051921780174\n",
      "Hamming loss  0.2696561024949427\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2335, Recall: 0.6089, F1-measure: 0.3375\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2244, Recall: 0.5908, F1-measure: 0.3171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.67      0.34       351\n",
      "           1       0.28      0.53      0.37       515\n",
      "           2       0.12      0.56      0.20       150\n",
      "           3       0.50      0.68      0.58       885\n",
      "           4       0.14      0.59      0.23       229\n",
      "           5       0.18      0.64      0.29       268\n",
      "           6       0.20      0.52      0.29       311\n",
      "           7       0.38      0.69      0.49       593\n",
      "           8       0.15      0.52      0.23       248\n",
      "           9       0.12      0.50      0.19       200\n",
      "          10       0.17      0.54      0.26       270\n",
      "          11       0.28      0.60      0.38       166\n",
      "          12       0.17      0.49      0.25       239\n",
      "          13       0.26      0.74      0.38       276\n",
      "          14       0.19      0.60      0.29       318\n",
      "\n",
      "   micro avg       0.23      0.61      0.34      5019\n",
      "   macro avg       0.22      0.59      0.32      5019\n",
      "weighted avg       0.27      0.61      0.37      5019\n",
      " samples avg       0.23      0.49      0.28      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:04.838110\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_charngram, y_train)\n",
    "predictions = classifier.predict(x_test_charngram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:01:20.744426\n"
     ]
    }
   ],
   "source": [
    "vectorizer_charngram = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char', ngram_range=(2, 4),  max_features=20000)\n",
    "x_train_charngram = vectorizer_charngram.fit_transform(train['cleaned'])\n",
    "x_test_charngram = vectorizer_charngram.transform(test['cleaned'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.037086985839514496\n",
      "Hamming loss  0.25567543268150145\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2410, Recall: 0.5890, F1-measure: 0.3420\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2255, Recall: 0.5577, F1-measure: 0.3149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.58      0.35       351\n",
      "           1       0.26      0.61      0.37       515\n",
      "           2       0.12      0.49      0.19       150\n",
      "           3       0.50      0.68      0.58       885\n",
      "           4       0.15      0.51      0.23       229\n",
      "           5       0.19      0.64      0.29       268\n",
      "           6       0.19      0.54      0.28       311\n",
      "           7       0.38      0.68      0.49       593\n",
      "           8       0.15      0.55      0.24       248\n",
      "           9       0.13      0.30      0.18       200\n",
      "          10       0.18      0.38      0.24       270\n",
      "          11       0.28      0.58      0.38       166\n",
      "          12       0.17      0.51      0.25       239\n",
      "          13       0.25      0.74      0.38       276\n",
      "          14       0.19      0.58      0.28       318\n",
      "\n",
      "   micro avg       0.24      0.59      0.34      5019\n",
      "   macro avg       0.23      0.56      0.31      5019\n",
      "weighted avg       0.27      0.59      0.37      5019\n",
      " samples avg       0.23      0.47      0.29      5019\n",
      "\n",
      "Time taken to run this cell : 0:00:03.188559\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_charngram, y_train)\n",
    "predictions = classifier.predict(x_test_charngram)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_char_6 = x_train_char_6.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9489, 20000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_char_6 .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_6 = x_test_char_6.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_6 = np.hstack((x_train_5, x_train_char_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_6 = np.hstack((x_test_5, x_test_char_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('final_train.npy', x_train_6)\n",
    "np.save('final_test.npy', x_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.08968307484828052\n",
      "Hamming loss  0.15652955720386605\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3892, Recall: 0.6808, F1-measure: 0.4953\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3645, Recall: 0.6481, F1-measure: 0.4599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.70      0.44       351\n",
      "           1       0.60      0.82      0.70       515\n",
      "           2       0.21      0.61      0.31       150\n",
      "           3       0.61      0.69      0.65       885\n",
      "           4       0.21      0.56      0.31       229\n",
      "           5       0.38      0.73      0.50       268\n",
      "           6       0.35      0.67      0.46       311\n",
      "           7       0.49      0.83      0.62       593\n",
      "           8       0.25      0.61      0.36       248\n",
      "           9       0.28      0.36      0.31       200\n",
      "          10       0.41      0.74      0.53       270\n",
      "          11       0.43      0.69      0.53       166\n",
      "          12       0.28      0.40      0.33       239\n",
      "          13       0.34      0.75      0.47       276\n",
      "          14       0.30      0.55      0.39       318\n",
      "\n",
      "   micro avg       0.39      0.68      0.50      5019\n",
      "   macro avg       0.36      0.65      0.46      5019\n",
      "weighted avg       0.42      0.68      0.51      5019\n",
      " samples avg       0.38      0.59      0.43      5019\n",
      "\n",
      "Time taken to run this cell : 0:03:22.685506\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_6, y_train)\n",
    "predictions = classifier.predict(x_test_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.19082939986513822\n",
      "Hamming loss  0.124881995954147\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4581, Recall: 0.5850, F1-measure: 0.5138\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4342, Recall: 0.5203, F1-measure: 0.4638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42       351\n",
      "           1       0.54      0.88      0.67       515\n",
      "           2       0.38      0.43      0.40       150\n",
      "           3       0.60      0.74      0.66       885\n",
      "           4       0.32      0.25      0.28       229\n",
      "           5       0.52      0.50      0.51       268\n",
      "           6       0.39      0.34      0.36       311\n",
      "           7       0.52      0.77      0.62       593\n",
      "           8       0.36      0.40      0.38       248\n",
      "           9       0.32      0.31      0.32       200\n",
      "          10       0.36      0.64      0.46       270\n",
      "          11       0.78      0.57      0.66       166\n",
      "          12       0.28      0.43      0.34       239\n",
      "          13       0.40      0.64      0.49       276\n",
      "          14       0.32      0.51      0.39       318\n",
      "\n",
      "   micro avg       0.46      0.58      0.51      5019\n",
      "   macro avg       0.43      0.52      0.46      5019\n",
      "weighted avg       0.46      0.58      0.51      5019\n",
      " samples avg       0.40      0.51      0.42      5019\n",
      "\n",
      "Time taken to run this cell : 0:02:51.137647\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.0001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_6, y_train)\n",
    "predictions = classifier.predict(x_test_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.25151719487525287\n",
      "Hamming loss  0.10570914812317375\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5405, Recall: 0.4198, F1-measure: 0.4726\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4959, Recall: 0.3713, F1-measure: 0.4141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.30      0.36       351\n",
      "           1       0.72      0.67      0.70       515\n",
      "           2       0.45      0.25      0.32       150\n",
      "           3       0.61      0.53      0.57       885\n",
      "           4       0.39      0.14      0.21       229\n",
      "           5       0.57      0.39      0.46       268\n",
      "           6       0.45      0.30      0.36       311\n",
      "           7       0.66      0.53      0.59       593\n",
      "           8       0.38      0.21      0.27       248\n",
      "           9       0.42      0.19      0.26       200\n",
      "          10       0.44      0.41      0.43       270\n",
      "          11       0.71      0.60      0.65       166\n",
      "          12       0.38      0.11      0.17       239\n",
      "          13       0.46      0.53      0.49       276\n",
      "          14       0.35      0.40      0.37       318\n",
      "\n",
      "   micro avg       0.54      0.42      0.47      5019\n",
      "   macro avg       0.50      0.37      0.41      5019\n",
      "weighted avg       0.53      0.42      0.46      5019\n",
      " samples avg       0.40      0.38      0.36      5019\n",
      "\n",
      "Time taken to run this cell : 0:05:26.553297\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_6, y_train)\n",
    "predictions = classifier.predict(x_test_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.12306136210384357\n",
      "Hamming loss  0.13639020004495392\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4297, Recall: 0.6392, F1-measure: 0.5139\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4046, Recall: 0.5886, F1-measure: 0.4728\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.62      0.47       351\n",
      "           1       0.61      0.84      0.71       515\n",
      "           2       0.38      0.39      0.38       150\n",
      "           3       0.58      0.72      0.65       885\n",
      "           4       0.24      0.37      0.29       229\n",
      "           5       0.37      0.70      0.48       268\n",
      "           6       0.34      0.65      0.45       311\n",
      "           7       0.57      0.75      0.65       593\n",
      "           8       0.32      0.44      0.37       248\n",
      "           9       0.27      0.31      0.29       200\n",
      "          10       0.42      0.74      0.54       270\n",
      "          11       0.63      0.63      0.63       166\n",
      "          12       0.31      0.40      0.35       239\n",
      "          13       0.29      0.73      0.42       276\n",
      "          14       0.34      0.53      0.42       318\n",
      "\n",
      "   micro avg       0.43      0.64      0.51      5019\n",
      "   macro avg       0.40      0.59      0.47      5019\n",
      "weighted avg       0.45      0.64      0.52      5019\n",
      " samples avg       0.39      0.57      0.42      5019\n",
      "\n",
      "Time taken to run this cell : 0:05:30.171112\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_6, y_train)\n",
    "predictions = classifier.predict(x_test_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.1830748482805125\n",
      "Hamming loss  0.12485951899303215\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4464, Recall: 0.4443, F1-measure: 0.4453\n",
      "Macro-average quality numbers\n",
      "Precision: 0.4296, Recall: 0.4241, F1-measure: 0.4106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.31      0.36       351\n",
      "           1       0.65      0.76      0.70       515\n",
      "           2       0.40      0.41      0.41       150\n",
      "           3       0.61      0.43      0.51       885\n",
      "           4       0.31      0.21      0.25       229\n",
      "           5       0.32      0.47      0.38       268\n",
      "           6       0.41      0.48      0.44       311\n",
      "           7       0.70      0.45      0.55       593\n",
      "           8       0.34      0.37      0.36       248\n",
      "           9       0.31      0.25      0.28       200\n",
      "          10       0.41      0.13      0.19       270\n",
      "          11       0.67      0.60      0.63       166\n",
      "          12       0.20      0.35      0.26       239\n",
      "          13       0.38      0.67      0.48       276\n",
      "          14       0.30      0.48      0.37       318\n",
      "\n",
      "   micro avg       0.45      0.44      0.45      5019\n",
      "   macro avg       0.43      0.42      0.41      5019\n",
      "weighted avg       0.48      0.44      0.44      5019\n",
      " samples avg       0.34      0.39      0.34      5019\n",
      "\n",
      "Time taken to run this cell : 0:03:38.496445\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.0001, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_6, y_train)\n",
    "predictions = classifier.predict(x_test_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.044504383007417395\n",
      "Hamming loss  0.2279838165879973\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2950, Recall: 0.7346, F1-measure: 0.4210\n",
      "Macro-average quality numbers\n",
      "Precision: 0.2974, Recall: 0.7303, F1-measure: 0.4081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.39       351\n",
      "           1       0.49      0.93      0.64       515\n",
      "           2       0.15      0.72      0.25       150\n",
      "           3       0.57      0.63      0.60       885\n",
      "           4       0.17      0.75      0.27       229\n",
      "           5       0.28      0.80      0.41       268\n",
      "           6       0.28      0.76      0.41       311\n",
      "           7       0.47      0.79      0.59       593\n",
      "           8       0.20      0.76      0.32       248\n",
      "           9       0.18      0.47      0.26       200\n",
      "          10       0.42      0.74      0.53       270\n",
      "          11       0.35      0.72      0.47       166\n",
      "          12       0.14      0.77      0.24       239\n",
      "          13       0.26      0.74      0.38       276\n",
      "          14       0.24      0.64      0.35       318\n",
      "\n",
      "   micro avg       0.30      0.73      0.42      5019\n",
      "   macro avg       0.30      0.73      0.41      5019\n",
      "weighted avg       0.36      0.73      0.46      5019\n",
      " samples avg       0.29      0.63      0.37      5019\n",
      "\n",
      "Time taken to run this cell : 0:02:35.288282\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.01, penalty='l2', class_weight=\"balanced\"), n_jobs = -1)\n",
    "classifier.fit(x_train_6, y_train)\n",
    "predictions = classifier.predict(x_test_6)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
